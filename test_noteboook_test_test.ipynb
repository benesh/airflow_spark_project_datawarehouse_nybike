{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a7caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"App_Benesh_\")\n",
    "\n",
    "\n",
    "latitude=40.7423543\n",
    "longitude=-73.98915076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10fdd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = geolocator.reverse(f\"{latitude},{longitude}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be571e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'place_id': 331978482, 'licence': 'Data Â© OpenStreetMap contributors, ODbL 1.0. http://osm.org/copyright', 'osm_type': 'node', 'osm_id': 10177726977, 'lat': '40.7423907', 'lon': '-73.9891937', 'class': 'amenity', 'type': 'waste_basket', 'place_rank': 30, 'importance': 9.175936522464359e-05, 'addresstype': 'amenity', 'name': '', 'display_name': 'Broadway, Flatiron District, Manhattan Community Board 5, Manhattan, New York County, City of New York, New York, 10019, United States', 'address': {'road': 'Broadway', 'neighbourhood': 'Flatiron District', 'borough': 'Manhattan', 'county': 'New York County', 'city': 'City of New York', 'state': 'New York', 'ISO3166-2-lvl4': 'US-NY', 'postcode': '10019', 'country': 'United States', 'country_code': 'us'}, 'boundingbox': ['40.7423407', '40.7424407', '-73.9892437', '-73.9891437']}\n"
     ]
    }
   ],
   "source": [
    "# print(location.address)\n",
    "\n",
    "# print((location.latitude, location.longitude))\n",
    "\n",
    "print(location.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226ca608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/06/17 10:21:09 WARN Utils: Your hostname, DESKTOP-2ELJ47S resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/06/17 10:21:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/17 10:21:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"spark-etl_nybike_bronze\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d8681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 17:18:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv('volumes_and_data_services/data_nybike/2013-citibike-tripdata/6_June/201306-citibike-tripdata_1.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec4c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "577703"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a5bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnsRenamed({'start station latitude':'start_latitude','start station longitude':'start_longitude','end station latitude':'end_latitude','end station longitude':'end_longitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dea9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp,col\n",
    "df=df.withColumn('starttime',to_timestamp(col('starttime')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa69597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 10:21:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(\"src/jobs/pyspark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12dd907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column With value literal initiated\n"
     ]
    }
   ],
   "source": [
    "from src.jobs.pyspark.interfaces import *\n",
    "from src.jobs.pyspark.transformers import *\n",
    "from pyspark.sql.functions import lit\n",
    "df = AddColumnWithLiteralValue().run(df,{'column_to_add':{'column_name':'genre_enr','column_value':'FEMMEUILE'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from src.jobs.pyspark.transformers import AddDimensionsForTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a609d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+--------------------+\n",
      "|tripduration|          starttime|           stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|             id_uuid|\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+--------------------+\n",
      "|         695|2013-06-01 00:00:01|2013-06-01 00:11:36|             444|  Broadway & W 24 St|            40.7423543|           -73.98915076|         434.0|     9 Ave & W 18 St|         40.74317449|         -74.00366443| 19678|Subscriber|    1983.0|     1|37952491-ef5d-4ad...|\n",
      "|         693|2013-06-01 00:00:08|2013-06-01 00:11:41|             444|  Broadway & W 24 St|            40.7423543|           -73.98915076|         434.0|     9 Ave & W 18 St|         40.74317449|         -74.00366443| 16649|Subscriber|    1984.0|     1|525837ce-1d59-469...|\n",
      "|        2059|2013-06-01 00:00:44|2013-06-01 00:35:03|             406|Hicks St & Montag...|           40.69512845|           -73.99595065|         406.0|Hicks St & Montag...|         40.69512845|         -73.99595065| 19599|  Customer|      NULL|     0|cab448fa-d66a-458...|\n",
      "|         123|2013-06-01 00:01:04|2013-06-01 00:03:07|             475| E 15 St & Irving Pl|           40.73524276|           -73.98758561|         262.0|     Washington Park|          40.6917823|          -73.9737299| 16352|Subscriber|    1960.0|     1|0099c750-1a31-4fe...|\n",
      "|        1521|2013-06-01 00:01:22|2013-06-01 00:26:43|            2008|Little West St & ...|           40.70569254|           -74.01677685|         310.0| State St & Smith St|         40.68926942|         -73.98912867| 15567|Subscriber|    1983.0|     1|f2d61d6c-136b-432...|\n",
      "|        2028|2013-06-01 00:01:47|2013-06-01 00:35:35|             485|     W 37 St & 5 Ave|           40.75038009|           -73.98338988|         406.0|Hicks St & Montag...|         40.69512845|         -73.99595065| 18445|  Customer|      NULL|     0|e0135bf5-f6db-473...|\n",
      "|        2057|2013-06-01 00:02:33|2013-06-01 00:36:50|             285|  Broadway & E 14 St|           40.73454567|           -73.99074142|         532.0|     S 5 Pl & S 5 St|           40.710451|           -73.960876| 15693|Subscriber|    1991.0|     1|ee974801-ecf8-423...|\n",
      "|         369|2013-06-01 00:03:29|2013-06-01 00:09:38|             509|     9 Ave & W 22 St|            40.7454973|           -74.00197139|         521.0|   8 Ave & W 31 St N|   40.75096734871598|   -73.99444207549095| 16100|Subscriber|    1981.0|     1|7e302e80-4885-45f...|\n",
      "|        1829|2013-06-01 00:03:47|2013-06-01 00:34:16|             265|Stanton St & Chry...|           40.72229346|           -73.99147535|         436.0|Hancock St & Bedf...|         40.68216564|         -73.95399026| 15234|Subscriber|    1984.0|     1|08ea5e7e-d073-440...|\n",
      "|         829|2013-06-01 00:04:22|2013-06-01 00:18:11|             404|     9 Ave & W 14 St|            40.7405826|           -74.00550867|         303.0|Mercer St & Sprin...|         40.72362738|         -73.99949601| 16400|Subscriber|    1987.0|     1|2c242e91-4bf4-4e7...|\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from pyspark.sql.functions import udf,coalesce,when,col,lit,hash,datediff,year,quarter,dayofmonth,month,dayofweek,date_format,concat,to_timestamp,udf,isnull ,when\n",
    "\n",
    "@udf\n",
    "def uuid_gen():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# df.select('*',uuid_gen().alias('id_uuid')).show(10)\n",
    "df.withColumn('id_uuid',uuid_gen()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8b12c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions time column add initiated\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+---------+----+-----+-------+---+-------+----------+------------+------------+\n",
      "|tripduration|          starttime|           stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|genre_enr|year|month|quarter|day|weekday|month_name|weekday_name|quarter_name|\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+---------+----+-----+-------+---+-------+----------+------------+------------+\n",
      "|         695|2013-06-01 00:00:01|2013-06-01 00:11:36|             444|  Broadway & W 24 St|            40.7423543|           -73.98915076|         434.0|     9 Ave & W 18 St|         40.74317449|         -74.00366443| 19678|Subscriber|    1983.0|     1|FEMMEUILE|2013|    6|      2|  1|      7|      June|    Saturday|      2013Q2|\n",
      "|         693|2013-06-01 00:00:08|2013-06-01 00:11:41|             444|  Broadway & W 24 St|            40.7423543|           -73.98915076|         434.0|     9 Ave & W 18 St|         40.74317449|         -74.00366443| 16649|Subscriber|    1984.0|     1|FEMMEUILE|2013|    6|      2|  1|      7|      June|    Saturday|      2013Q2|\n",
      "|        2059|2013-06-01 00:00:44|2013-06-01 00:35:03|             406|Hicks St & Montag...|           40.69512845|           -73.99595065|         406.0|Hicks St & Montag...|         40.69512845|         -73.99595065| 19599|  Customer|      NULL|     0|FEMMEUILE|2013|    6|      2|  1|      7|      June|    Saturday|      2013Q2|\n",
      "|         123|2013-06-01 00:01:04|2013-06-01 00:03:07|             475| E 15 St & Irving Pl|           40.73524276|           -73.98758561|         262.0|     Washington Park|          40.6917823|          -73.9737299| 16352|Subscriber|    1960.0|     1|FEMMEUILE|2013|    6|      2|  1|      7|      June|    Saturday|      2013Q2|\n",
      "|        1521|2013-06-01 00:01:22|2013-06-01 00:26:43|            2008|Little West St & ...|           40.70569254|           -74.01677685|         310.0| State St & Smith St|         40.68926942|         -73.98912867| 15567|Subscriber|    1983.0|     1|FEMMEUILE|2013|    6|      2|  1|      7|      June|    Saturday|      2013Q2|\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+---------+----+-----+-------+---+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.jobs.pyspark.interfaces import *\n",
    "from src.jobs.pyspark.transformers import AddDimensionsForTimesv2\n",
    "from pyspark.sql.functions import *\n",
    "config={'datetime_column':'starttime'}\n",
    "# df.show(1)\n",
    "AddDimensionsForTimesv2().run(df,config).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97fecd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add uuid in to th column: uuid_id\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+---------+--------------------+\n",
      "|tripduration|          starttime|           stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|genre_enr|             uuid_id|\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+---------+--------------------+\n",
      "|         695|2013-06-01 00:00:01|2013-06-01 00:11:36|             444|  Broadway & W 24 St|            40.7423543|           -73.98915076|         434.0|     9 Ave & W 18 St|         40.74317449|         -74.00366443| 19678|Subscriber|    1983.0|     1|FEMMEUILE|f92ee793-b450-490...|\n",
      "|         693|2013-06-01 00:00:08|2013-06-01 00:11:41|             444|  Broadway & W 24 St|            40.7423543|           -73.98915076|         434.0|     9 Ave & W 18 St|         40.74317449|         -74.00366443| 16649|Subscriber|    1984.0|     1|FEMMEUILE|6dbbe671-4ff3-4a3...|\n",
      "|        2059|2013-06-01 00:00:44|2013-06-01 00:35:03|             406|Hicks St & Montag...|           40.69512845|           -73.99595065|         406.0|Hicks St & Montag...|         40.69512845|         -73.99595065| 19599|  Customer|      NULL|     0|FEMMEUILE|c05fc4f7-d047-420...|\n",
      "|         123|2013-06-01 00:01:04|2013-06-01 00:03:07|             475| E 15 St & Irving Pl|           40.73524276|           -73.98758561|         262.0|     Washington Park|          40.6917823|          -73.9737299| 16352|Subscriber|    1960.0|     1|FEMMEUILE|f5deb0b2-8c98-401...|\n",
      "|        1521|2013-06-01 00:01:22|2013-06-01 00:26:43|            2008|Little West St & ...|           40.70569254|           -74.01677685|         310.0| State St & Smith St|         40.68926942|         -73.98912867| 15567|Subscriber|    1983.0|     1|FEMMEUILE|b47ce78b-c829-46b...|\n",
      "+------------+-------------------+-------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.jobs.pyspark.interfaces import *\n",
    "from src.jobs.pyspark.transformers import AddUuidToColumnID\n",
    "from pyspark.sql.functions import *\n",
    "config={'column_id':'uuid_id'}\n",
    "# df.show(1)\n",
    "AddUuidToColumnID().run(df,config).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a36fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql.functions import udf,coalesce,when,col,lit,hash,datediff,year,quarter,dayofmonth,month,dayofweek,date_format,concat,to_timestamp,udf,isnull ,when,col\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"App_Benesh_\")\n",
    "\n",
    "@udf\n",
    "def enrich_location(longitude,latitude): \n",
    "    geolocator = Nominatim(user_agent=\"App_Benesh_\")\n",
    "    location = geolocator.reverse(f\"{latitude},{longitude}\")\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31cd3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('*',enrich_location(col(\"start_latitude\"),col(\"start_longitude\")).alias(\"enr_start_location\"), enrich_location(col(\"end_latitude\"),col(\"end_longitude\")).alias(\"enr_end_location\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea15195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 17:04:04 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:34423\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/06/16 17:04:04 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:34423\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "# df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c87b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chispa.dataframe_comparer import *\n",
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType , TimestampType , DoubleType , FloatType, DateType\n",
    "from pyspark.sql.functions import udf,coalesce,when,col,lit,year,quarter,dayofmonth,month,dayofweek,date_format,concat,to_timestamp,udf,isnull ,when\n",
    "\n",
    "from interfaces import DataTransformer\n",
    "from transformers import RenameColumn,AddColumnDiffTime,DropColumns,CastToTimestamp\n",
    "from model_data import bronze_schema_ny_bike\n",
    "from helpers_utils import config_reader\n",
    "\n",
    "input_data = [{\n",
    "            \"start_at\":\"1/1/2015 0:01\",\n",
    "            \"stop_at\":\"1/1/2015 0:24\"\n",
    "            }]\n",
    "\n",
    "expected_data = [{\n",
    "            \"start_at\":\"2015-1-01 0:01:00\",\n",
    "            \"stop_at\":\"2015-1-01 0:24:00\"             \n",
    "            }]\n",
    "\n",
    "\n",
    "config={\n",
    "    \"cast_to_timestamp\":[\"start_at\",\"stop_at\"]\n",
    "}\n",
    "transformer_cast = CastToTimestamp()\n",
    "df_input_data=spark.createDataFrame(input_data)\n",
    "df_expected_data=spark.createDataFrame(expected_data)\n",
    "\n",
    "\n",
    "# assertDataFrameEqual(result_transformation,df_expected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a0648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cast to timestamp: ['start_at', 'stop_at']\n"
     ]
    }
   ],
   "source": [
    "df_expected_data = df_expected_data\\\n",
    "    .withColumn('start_at',to_timestamp(col('start_at')))\\\n",
    "    .withColumn('stop_at',to_timestamp(col('stop_at')))\n",
    "result_transformation = transformer_cast.run(df_input_data,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2666d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|      start_at|       stop_at|\n",
      "+--------------+--------------+\n",
      "|,1/1/2015 0:01|,1/1/2015 0:24|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197f644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           start_at|            stop_at|\n",
      "+-------------------+-------------------+\n",
      "|2015-01-01 00:01:00|2015-01-01 00:24:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_transformation.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl-datawahouse-nybike-04ju5hT8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
