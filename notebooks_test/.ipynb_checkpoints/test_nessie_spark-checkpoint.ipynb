{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f513f406-5928-4473-b140-82c95d2a5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2d4236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://minio:9000\n",
      "http://nessie:19120/api/v1\n",
      "s3a://warehouse/\n",
      "M1s2Sa2IecSYl6SR6n4W\n",
      "Kj6gSYsC1Q5MJ6VXWVW1S1m8eC8gPKxtLcOxB2wk\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "NESSIE_URI = os.environ.get(\"NESSIE_URI\") ## Nessie Server URI\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\") ## BUCKET TO WRITE DATA TOO\n",
    "AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\") ## AWS CREDENTIALS\n",
    "AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") ## AWS CREDENTIALS\n",
    "AWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "\n",
    "\n",
    "print(AWS_S3_ENDPOINT)\n",
    "print(NESSIE_URI)\n",
    "print(WAREHOUSE)\n",
    "print(AWS_ACCESS_KEY_ID)\n",
    "print(AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ea8a43-9c5e-4bd0-a0e2-019fcdf37fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS_REGION is used by Spark\n",
    "AWS_REGION=\"us-east-1\"\n",
    "# This must match if using minio\n",
    "MINIO_REGION=\"us-east-1\"\n",
    "# Used by pyIceberg\n",
    "AWS_DEFAULT_REGION=\"us-east-1\"\n",
    "# AWS Credentials (this can use minio credential, to be filled in later)\n",
    "AWS_ACCESS_KEY_ID=\"YRUl5sf7KF7WTVAAgTSO\"\n",
    "AWS_SECRET_ACCESS_KEY=\"gWIjs4ljGZJHcum9RlZtKTrL5La7FNWXZvwppm6L\"\n",
    "# If using Minio, this should be the API address of Minio Server\n",
    "AWS_S3_ENDPOINT=\"http://minio:9000\"\n",
    "# Location where files will be written when creating new tables\n",
    "WAREHOUSE=\"s3a://warehouse/\"\n",
    "# URI of Nessie Catalog\n",
    "NESSIE_URI=\"http://nessie:19120/api/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847dcef6-5174-4deb-b402-5ed9c3d5a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0a18b936-b508-4b25-a989-c5beaf22adb8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.102.5 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.131 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.20.131 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.131 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.131 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.131 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.131 in central\n",
      ":: resolution report :: resolve 464ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.102.5 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0a18b936-b508-4b25-a989-c5beaf22adb8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/14ms)\n",
      "25/04/10 15:35:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.bronze', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.bronze.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.bronze.ref', 'main')\n",
    "        .set('spark.sql.catalog.bronze.authentication.type', 'NONE')  # ✅ Move auth to \"bronze\" catalog\n",
    "        .set('spark.sql.catalog.bronze.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.sql.catalog.bronze.s3.path-style-access', 'true')  # ✅ Configure S3 for \"bronze\"\n",
    "        .set('spark.sql.catalog.bronze.s3.endpoint', AWS_S3_ENDPOINT)  # ✅ Fixed typo (no 's' at end)\n",
    "        .set('spark.sql.catalog.bronze.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.bronze.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access','true')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    ")\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3057f689-5f2a-4fb0-a4d6-1869c5ccc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri',\"http://nessie:19120/api/v1\" )\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint',\"http://minio:9000\")\n",
    "        .set('spark.sql.catalog.nessie.warehouse', \"s3://warehouse/\")\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.hadoop.fs.s3a.access.key',\"YRUl5sf7KF7WTVAAgTSO\" )\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', \"gWIjs4ljGZJHcum9RlZtKTrL5La7FNWXZvwppm6L\")\n",
    "        .set(\"spark.hadoop.fs.s3.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    ")\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3529b338-7080-471c-acdd-a34751ebceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', \"http://nessie:19120/api/v1\")\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', \"http://minio:9000\")  # ✅ Fixed key\n",
    "        .set('spark.sql.catalog.nessie.warehouse', \"s3://warehouse/\")\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.sql.catalog.nessie.s3.path-style-access', 'true')  # ✅ Enforce path-style\n",
    "        .set('spark.hadoop.fs.s3a.access.key', \"YRUl5sf7KF7WTVAAgTSO\")\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', \"gWIjs4ljGZJHcum9RlZtKTrL5La7FNWXZvwppm6L\")\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    ")\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d4e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8332c45-5c3c-4c6e-b600-e06dc3279389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get('spark.sql.catalog.nessie.s3.endpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d022875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|DW_ny_bike|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create a Table\n",
    "spark.sql(\"show databases in bronze;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f56016-399a-49ce-a1bf-a18d170a0664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a Table\n",
    "spark.sql(\" CREATE DATABASE dw_nybike;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3265b6-3301-48cd-8be9-420b7c99ee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a Table\n",
    "spark.sql(\" CREATE DATABASE IF NOT EXISTS bronze.DW_ny_bike;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf18acb-7e6b-4ac3-a04a-73b2c7fdf11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|dw_nybike|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases in nessie;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee6124f-990c-418f-baab-b707aabda202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "| current-snapshot-id|                none|\n",
      "|              format|     iceberg/parquet|\n",
      "|      format-version|                   2|\n",
      "|          gc.enabled|               false|\n",
      "|    nessie.commit.id|b40136b3c0da8bffe...|\n",
      "|write.metadata.de...|               false|\n",
      "|write.parquet.com...|                zstd|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"show tables in nessie.dw_nybike;\").show()\n",
    "spark.sql(\"SHOW TBLPROPERTIES nessie.dw_nybike.trip_data_nybike_v2;\").show()\n",
    "# spark.sql(\"SELECT * FROM nessie.dw_nybike.trip_data_nybike_v2.snapshots;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6de411a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE bronze.DW_ny_bike.trip_data_nybike(\n",
    "    dw_period_tag string,\n",
    "    ride_id string,\n",
    "\tstart_station_id string,\n",
    "\tstart_station_name string,\n",
    "\tstart_station_latitude string ,\n",
    "\tstart_station_longitude string ,\n",
    "\tend_station_id string,\n",
    "\tend_station_name string,\n",
    "\tend_station_latitude string,\n",
    "\tend_station_longitude string,\n",
    "\tuser_type string,\n",
    "    gender string,\n",
    "\tcustomer_year_birth string,\n",
    "    bike_id string,\n",
    "\trideable_type string,\n",
    "\tstart_at string,\n",
    "\tstop_at string, \n",
    "\ttrip_duration string\n",
    "    )\n",
    "USING iceberg\n",
    "PARTITIONED BY (dw_period_tag)\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fd06fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"SHOW tables in bronze.DW_ny_bike;\").show()\n",
    "# spark.sql(\"ALTER TABLE bronze.DW_ny_bike.trip_data_nybike ALTER COLUMN start_at TYPE string;\")\n",
    "spark.sql(\"DROP TABLE bronze.DW_ny_bike.trip_data_nybike;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c383c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from  bronze.DW_ny_bike.trip_data_nybike limit 10;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf20397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             name|\n",
      "+-----------------+\n",
      "|      Alex Merced|\n",
      "|Dipankar Mazumdar|\n",
      "|     Jason Hughes|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Insert Some Data\n",
    "spark.sql(\"INSERT INTO nessie.names VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2308fd5-e767-42aa-8b69-5649b40153d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "|dw_period_tag|ride_id|start_station_id|  start_station_name|start_station_latitude|start_station_longitude|end_station_id|    end_station_name|end_station_latitude|end_station_longitude| user_type|gender|customer_year_birth|bike_id|rideable_type|start_at|stop_at|trip_duration|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "|         2014|   NULL|             479|     9 Ave & W 45 St|           40.76019252|            -73.9912551|           540|Lexington Ave & E...|         40.74147286|         -73.98320928|Subscriber|     1|             1977.0|  21376|         NULL|    NULL|   NULL|       1027.0|\n",
      "|         2014|   NULL|             417|Barclay St & Chur...|           40.71291224|           -74.01020234|           417|Barclay St & Chur...|         40.71291224|         -74.01020234|Subscriber|     2|             1974.0|  16086|         NULL|    NULL|   NULL|        534.0|\n",
      "|         2014|   NULL|             327|Vesey Pl & River ...|            40.7153379|           -74.01658354|           415|Pearl St & Hanove...|          40.7047177|         -74.00926027|Subscriber|     1|             1990.0|  16073|         NULL|    NULL|   NULL|        416.0|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Query the Data\n",
    "spark.sql(\"SELECT * FROM nessie.dw_nybike.trip_data_nybike limit 3;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5e4f1d9-d582-4668-b8d2-7988cce18c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame \n",
    "from typing import Optional\n",
    "from pyspark.sql.functions import lit,concat,to_timestamp,unix_timestamp , from_unixtime ,try_to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType , TimestampType , DoubleType , FloatType, DateType\n",
    "\n",
    "bronze_schema_ny_bike = StructType([\n",
    "    StructField(\"dw_period_tag\", StringType(), nullable=True),\n",
    "    StructField(\"ride_id\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_id\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_name\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_latitude\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_longitude\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_id\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_name\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_latitude\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_longitude\", StringType(), nullable=True),\n",
    "    StructField(\"bike_id\", StringType(), nullable=True),\n",
    "    StructField(\"user_type\", StringType(), nullable=True),\n",
    "    StructField(\"gender\", StringType(), nullable=True),\n",
    "    StructField(\"customer_year_birth\", StringType(), nullable=True),\n",
    "    StructField(\"rideable_type\", StringType(), nullable=True),\n",
    "    StructField(\"start_at\", StringType(), nullable=True),\n",
    "    StructField(\"stop_at\", StringType(), nullable=True),\n",
    "    StructField(\"trip_duration\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c393fc1e-ec73-4a7d-a16b-9233a353dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def config_reader(path:str):\n",
    "    # Load YAML config\n",
    "    with open(path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7502a4b-52fe-4de0-b731-cde010db868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_bike_2014='/opt/airflow/data/2014-citibike-tripdata/10_October/*.csv'\n",
    "file_bike_2017='/opt/airflow/data/2017-citibike-tripdata/*/*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5439a662-bed2-4c5c-b4d3-1ef5851d63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2014 =spark.read.format('csv').options(inferSchema='True', header='True',delimiter=',').load(file_bike_2014)\n",
    "df2017 =spark.read.format('csv').options(header='True').load(file_bike_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b9381ef-9b4d-4c42-b4eb-9d9ef9aba355",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=config_reader('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90c9aad1-77f0-4e05-8643-1efdd9699bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripduration: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- stoptime: string (nullable = true)\n",
      " |-- start station id: string (nullable = true)\n",
      " |-- start station name: string (nullable = true)\n",
      " |-- start station latitude: string (nullable = true)\n",
      " |-- start station longitude: string (nullable = true)\n",
      " |-- end station id: string (nullable = true)\n",
      " |-- end station name: string (nullable = true)\n",
      " |-- end station latitude: string (nullable = true)\n",
      " |-- end station longitude: string (nullable = true)\n",
      " |-- bikeid: string (nullable = true)\n",
      " |-- usertype: string (nullable = true)\n",
      " |-- birth year: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2017.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14628d30-8c25-4cda-b234-15b1a0da7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed=df2016.withColumnsRenamed(config['etl_conf']['columns_to_rename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66377814-979a-48f0-93dc-eb209c60c9c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_COLUMN_OR_FIELD_DATA_TYPE] Column or field `start_station_latitude` is of type \"STRING\" while it's required to be \"DOUBLE\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_converted\u001b[38;5;241m=\u001b[39m\u001b[43mdf_renamed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbronze_schema_ny_bike\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:2293\u001b[0m, in \u001b[0;36mDataFrame.to\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m jschema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39msparkSession()\u001b[38;5;241m.\u001b[39mparseDataType(schema\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m-> 2293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjschema\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_COLUMN_OR_FIELD_DATA_TYPE] Column or field `start_station_latitude` is of type \"STRING\" while it's required to be \"DOUBLE\"."
     ]
    }
   ],
   "source": [
    "df_converted=df_renamed.to(bronze_schema_ny_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "729fbeaf-9e5b-4ec7-8ecc-7adc1f1b6a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_duration: string (nullable = true)\n",
      " |-- start_at: string (nullable = true)\n",
      " |-- stop_at: string (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: string (nullable = true)\n",
      " |-- start_station_longitude: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: string (nullable = true)\n",
      " |-- end_station_longitude: string (nullable = true)\n",
      " |-- bike_id: string (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- customer_year_birth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5522375-0652-4559-b55c-781e780bff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- start_at: string (nullable = true)\n",
      " |-- stop_at: string (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: double (nullable = true)\n",
      " |-- start_station_longitude: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: double (nullable = true)\n",
      " |-- end_station_longitude: double (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- customer_year_birth: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabd9d38-e317-41fa-89ce-a960461408d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfconverted = df.to(bronze_schema_ny_bike)\n",
    "# df.select(df.start_at.cast(\"TIMESTAMP\").alias('start_at')).show()\n",
    "# df.select(to_timestamp(df.start_at, 'MM/dd/yyyy HH:mm:ss')).show(5)\n",
    "# df.select(from_unixtime(unix_timestamp('start_at', 'MM/dd/yyyy HH:mm:ss')).cast(TimestampType()).alias(\"timestamp\")).show(5)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime,to_timestamp\n",
    "\n",
    "# df_renamed_v2 = df_renamed.select(from_unixtime(unix_timestamp('start_at', 'MM/dd/yyyy hh:mm:ss ')).cast(TimestampType()).alias(\"timestamp\"))\n",
    "# change timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2327764-65ab-43c3-bb52-6c66c3efac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed_v2 = df_renamed.withColumn(\"timestamp\", to_timestamp(\"start_at\", \"d/M/yyyy HH:mm:ss\"))\n",
    "# df = df.withColumn(\"timestamp\", to_timestamp(df[\"date_string\"], \"M/d/yyyy HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aedc22bb-1459-41f4-9141-9cd9827ca3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- start_at: string (nullable = true)\n",
      " |-- stop_at: string (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: double (nullable = true)\n",
      " |-- start_station_longitude: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: double (nullable = true)\n",
      " |-- end_station_longitude: double (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- customer_year_birth: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed_v2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4827c74a-c2b9-4780-afde-07f8dce7d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2014, 10, 1, 0, 0, 27)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "inDate = \"10/1/2014 00:00:27\"\n",
    "d = datetime.strptime(inDate, \"%m/%d/%Y %H:%M:%S\")\n",
    "d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21586738-a80b-47fc-a1c6-a47bf6b2365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime(2013, 4, 29, 15, 59, 2)\n",
    "d.strftime(\"YYYYMMDD HH:mm:ss (%Y%m%d %H:%M:%S)\")\n",
    "'YYYYMMDD HH:mm:ss (20130429 15:59:02)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
