{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f513f406-5928-4473-b140-82c95d2a5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2d4236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://minio:9000\n",
      "http://nessie:19120/api/v1\n",
      "s3a://warehouse/\n",
      "M1s2Sa2IecSYl6SR6n4W\n",
      "Kj6gSYsC1Q5MJ6VXWVW1S1m8eC8gPKxtLcOxB2wk\n",
      "s3a://sylver-warehouse/\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "NESSIE_URI = os.environ.get(\"NESSIE_URI\") ## Nessie Server URI\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\") ## BUCKET TO WRITE DATA TOO\n",
    "WAREHOUSE_BRONZE = os.environ.get(\"WAREHOUSE_BRONZE\") ## BUCKET TO WRITE DATA TOO\n",
    "WAREHOUSE_SYLVER = os.environ.get(\"WAREHOUSE_SYLVER\") ## BUCKET TO WRITE DATA TOO\n",
    "WAREHOUSE_GOLD = os.environ.get(\"WAREHOUSE_GOLD\") ## BUCKET TO WRITE DATA TOO\n",
    "AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\") ## AWS CREDENTIALS\n",
    "AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") ## AWS CREDENTIALS\n",
    "AWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "\n",
    "\n",
    "print(AWS_S3_ENDPOINT)\n",
    "print(NESSIE_URI)\n",
    "print(WAREHOUSE)\n",
    "print(AWS_ACCESS_KEY_ID)\n",
    "print(AWS_SECRET_ACCESS_KEY)\n",
    "print(WAREHOUSE_SYLVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ea8a43-9c5e-4bd0-a0e2-019fcdf37fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS_REGION is used by Spark\n",
    "AWS_REGION=\"us-east-1\"\n",
    "# This must match if using minio\n",
    "MINIO_REGION=\"us-east-1\"\n",
    "# Used by pyIceberg\n",
    "AWS_DEFAULT_REGION=\"us-east-1\"\n",
    "# AWS Credentials (this can use minio credential, to be filled in later)\n",
    "AWS_ACCESS_KEY_ID=\"YRUl5sf7KF7WTVAAgTSO\"\n",
    "AWS_SECRET_ACCESS_KEY=\"gWIjs4ljGZJHcum9RlZtKTrL5La7FNWXZvwppm6L\"\n",
    "# If using Minio, this should be the API address of Minio Server\n",
    "AWS_S3_ENDPOINT=\"http://minio:9000\"\n",
    "# Location where files will be written when creating new tables\n",
    "WAREHOUSE=\"s3a://warehouse/\"\n",
    "# URI of Nessie Catalog\n",
    "NESSIE_URI=\"http://nessie:19120/api/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db1d774-5d1d-4fd1-afed-4a6e683ed4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### not use\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.warehouse', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.warehouse.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.warehouse.ref', 'main')\n",
    "        .set('spark.sql.catalog.warehouse.authentication.type', 'NONE')  # ✅ Move auth to \"bronze\" catalog\n",
    "        .set('spark.sql.catalog.warehouse.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.sql.catalog.warehouse.s3.path-style-access', 'true')  # ✅ Configure S3 for \"bronze\"\n",
    "        .set('spark.sql.catalog.warehouse.s3.endpoint', AWS_S3_ENDPOINT)  # ✅ Fixed typo (no 's' at end)\n",
    "        .set('spark.sql.catalog.warehouse.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.warehouse.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access','true')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847dcef6-5174-4deb-b402-5ed9c3d5a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "## base config\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access','true')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bbfda89-462b-4332-adae-d995b75929bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fa338b5f370>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## config bronze\n",
    "\n",
    "conf.set('spark.sql.catalog.bronze', 'org.apache.iceberg.spark.SparkCatalog')\\\n",
    ".set('spark.sql.catalog.bronze.uri', NESSIE_URI)\\\n",
    ".set('spark.sql.catalog.bronze.ref', 'main')\\\n",
    ".set('spark.sql.catalog.bronze.authentication.type', 'NONE')\\\n",
    ".set('spark.sql.catalog.bronze.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\\\n",
    ".set('spark.sql.catalog.bronze.s3.path-style-access', 'true')\\\n",
    ".set('spark.sql.catalog.bronze.s3.endpoint', AWS_S3_ENDPOINT)\\\n",
    ".set('spark.sql.catalog.bronze.warehouse', WAREHOUSE_BRONZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f96f823-b83f-4ec0-bb2a-cd3f3353edf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fa35177d3c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sylver settings catalog\n",
    "\n",
    "conf.set('spark.sql.catalog.sylver', 'org.apache.iceberg.spark.SparkCatalog')\\\n",
    "    .set('spark.sql.catalog.sylver.uri',NESSIE_URI)\\\n",
    "    .set('spark.sql.catalog.sylver.ref', 'main')\\\n",
    "    .set('spark.sql.catalog.sylver.authentication.type', 'NONE')\\\n",
    "    .set('spark.sql.catalog.sylver.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\\\n",
    "    .set('spark.sql.catalog.sylver.s3.path-style-access', 'true')\\\n",
    "    .set('spark.sql.catalog.sylver.s3.endpoint',AWS_S3_ENDPOINT)\\\n",
    "    .set('spark.sql.catalog.sylver.warehouse',WAREHOUSE_BRONZE)\\\n",
    "    .set('spark.sql.catalog.sylver.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c7a61d9-2d3d-4063-a583-73ae25f72a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bronze settings catalog\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.sylver', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.sylver.uri',NESSIE_URI)\n",
    "        .set('spark.sql.catalog.sylver.ref', 'main')\n",
    "        .set('spark.sql.catalog.sylver.authentication.type', 'NONE')  # ✅ Move auth to \"bronze\" catalog\n",
    "        .set('spark.sql.catalog.sylver.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.sql.catalog.sylver.s3.path-style-access', 'true')  # ✅ Configure S3 for \"bronze\"\n",
    "        .set('spark.sql.catalog.sylver.s3.endpoint',AWS_S3_ENDPOINT)  # ✅ Fixed typo (no 's' at end)\n",
    "        .set('spark.sql.catalog.sylver.warehouse',WAREHOUSE_BRONZE)\n",
    "        .set('spark.sql.catalog.sylver.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.hadoop.fs.s3a.access.key',AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key',AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access','true')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e9b5e28-3fe9-4756-ba38-f80d4f2de064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sylver settings catalog\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.sylver', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.sylver.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.sylver.ref', 'main')\n",
    "        .set('spark.sql.catalog.sylver.authentication.type', 'NONE')  # ✅ Move auth to \"bronze\" catalog\n",
    "        .set('spark.sql.catalog.sylver.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.sql.catalog.sylver.s3.path-style-access', 'true')  # ✅ Configure S3 for \"bronze\"\n",
    "        .set('spark.sql.catalog.sylver.s3.endpoint', AWS_S3_ENDPOINT)  # ✅ Fixed typo (no 's' at end)\n",
    "        .set('spark.sql.catalog.sylver.warehouse', WAREHOUSE_SYLVER)\n",
    "        .set('spark.sql.catalog.sylver.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access','true')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e27fdf3a-a6d4-4df7-bfa0-d56bd28459be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gold settings catalog\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131')\n",
    "        .set('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.sylver', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.sylver.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.sylver.ref', 'main')\n",
    "        .set('spark.sql.catalog.sylver.authentication.type', 'NONE')  # ✅ Move auth to \"bronze\" catalog\n",
    "        .set('spark.sql.catalog.sylver.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.sql.catalog.sylver.s3.path-style-access', 'true')  # ✅ Configure S3 for \"bronze\"\n",
    "        .set('spark.sql.catalog.sylver.s3.endpoint', AWS_S3_ENDPOINT)  # ✅ Fixed typo (no 's' at end)\n",
    "        .set('spark.sql.catalog.sylver.warehouse', WAREHOUSE_GOLD)\n",
    "        .set('spark.sql.catalog.sylver.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')  # ✅ Use \"bronze\" prefix\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access','true')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98faed3-7f60-4205-a835-fc0bfa3dcfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-37c99170-7988-48eb-abef-fb0bc97ffd4b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.102.5 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.131 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.20.131 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.131 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.131 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.131 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.131 in central\n",
      ":: resolution report :: resolve 578ms :: artifacts dl 52ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.102.5 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.131 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-37c99170-7988-48eb-abef-fb0bc97ffd4b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/21ms)\n",
      "25/05/20 22:19:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68d4e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e044ba6-0f22-4d4d-b559-fe1596842d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5d1dcc7a84b5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app_name</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd105bcf550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8332c45-5c3c-4c6e-b600-e06dc3279389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get('spark.sql.catalog.nessie.s3.endpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d022875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create a Table\n",
    "spark.sql(\"show databases in warehouse;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3f56016-399a-49ce-a1bf-a18d170a0664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a Table\n",
    "spark.sql(\" CREATE DATABASE warehouse.gold;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c3265b6-3301-48cd-8be9-420b7c99ee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a Table\n",
    "spark.sql(\" CREATE DATABASE IF NOT EXISTS sylver.DW_ny_bike;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cf18acb-7e6b-4ac3-a04a-73b2c7fdf11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|   sylver|trip_data_nybike|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES in warehouse.sylver;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35cc5f47-e8b0-4cd3-a19b-35e1d7b34d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE warehouse.sylver.trip_data_nybike;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee6124f-990c-418f-baab-b707aabda202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "| current-snapshot-id| 5479237530675047789|\n",
      "|              format|     iceberg/parquet|\n",
      "|      format-version|                   2|\n",
      "|          gc.enabled|               false|\n",
      "|    nessie.commit.id|5133ffd6f336e8366...|\n",
      "|write.metadata.de...|               false|\n",
      "|write.parquet.com...|                zstd|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"show tables in nessie.dw_nybike;\").show()\n",
    "spark.sql(\"SHOW TBLPROPERTIES bronze.SylverDw_nybike.trip_data_nybike;\").show()\n",
    "# spark.sql(\"SELECT * FROM nessie.dw_nybike.trip_data_nybike_v2.snapshots;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6de411a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.bronze.trip_data_nybike(\n",
    "    dw_period_tag string,\n",
    "    ride_id string,\n",
    "\tstart_station_id string,\n",
    "\tstart_station_name string,\n",
    "\tstart_station_latitude string ,\n",
    "\tstart_station_longitude string ,\n",
    "\tend_station_id string,\n",
    "\tend_station_name string,\n",
    "\tend_station_latitude string,\n",
    "\tend_station_longitude string,\n",
    "\tuser_type string,\n",
    "    gender string,\n",
    "\tcustomer_year_birth string,\n",
    "    bike_id string,\n",
    "\trideable_type string,\n",
    "\tstart_at string,\n",
    "\tstop_at string, \n",
    "\ttrip_duration string\n",
    "    )\n",
    "USING iceberg\n",
    "PARTITIONED BY (dw_period_tag)\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93abdc4a-7cd9-4aa6-95b3-0a9c80a35544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.sylver.trip_data_nybike(\n",
    "    trip_uuid string,\n",
    "    dw_period_tag string,\n",
    "\tstart_station_id string,\n",
    "\tstart_station_name string,\n",
    "\tstart_station_latitude string,\n",
    "\tstart_station_longitude string,\n",
    "\tend_station_id string,\n",
    "\tend_station_name string,\n",
    "\tend_station_latitude string,\n",
    "\tend_station_longitude string,\n",
    "\tbike_id string,\n",
    "\tuser_type string,\n",
    "    enr_user_type string,\n",
    "    gender integer,\n",
    "    enr_gender string,\n",
    "\tcustomer_year_birth  string,\n",
    "\trideable_type string,\n",
    "\tenr_rideable_type string,\n",
    "\tstart_at timestamp,\n",
    "\tstop_at timestamp,\n",
    "\ttrip_duration double,\n",
    "\tenr_trip_duration double,\n",
    "    enr_year integer,\n",
    "    enr_quarter integer,\n",
    "    enr_quarter_name string,\n",
    "    enr_month integer,\n",
    "    enr_month_name string,\n",
    "    enr_day integer,\n",
    "    enr_weekday integer,\n",
    "    enr_weekday_name string\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (enr_year,enr_quarter,enr_month,bucket(10,enr_day));\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8579466f-b7c4-4645-90ef-11de9fc3232c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.gold.dim_rideable(\n",
    "    rideable_type_id integer,\n",
    "\trideable_type string\n",
    ")\n",
    "USING iceberg\n",
    ";\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d18f3-5514-4dfa-80c8-16f5b46efa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gold LAYER \n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.gold.fact_trip(\n",
    "    fact_id_uuid string, \n",
    "    dim_rideable_fk INTEGER,\n",
    "    start_at timestamp,\n",
    "\tstop_at timestamp,\n",
    "\ttrip_duration double,\n",
    "    enr_trip_duration double\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY(year(start_at),dim_rideable_fk)\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.gold.dim_location(\n",
    "    dim_location_uuid_id string,\n",
    "\tstart_station_id string,\n",
    "\tstart_station_name string,\n",
    "\tstart_station_latitude string,\n",
    "\tstart_station_longitude string,\n",
    "\tend_station_id string,\n",
    "\tend_station_name string,\n",
    "\tend_station_latitude string,\n",
    "\tend_station_longitude string,\n",
    "    start_at timestamp\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY(year(start_at))\n",
    ";\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.gold.dim_customer(\n",
    "    dim_customer_uuid string,\n",
    "\tuser_type string,\n",
    "    enr_user_type string,\n",
    "    gender integer,\n",
    "    enr_gender string,\n",
    "\tcustomer_year_birth  integer\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY(enr_user_type,enr_gender)\n",
    ";\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.gold.dim_times(\n",
    "    dim_time_uuid string,\n",
    "    enr_year integer,\n",
    "    enr_quarter integer,\n",
    "    enr_quarter_name string,\n",
    "    enr_month integer,\n",
    "    enr_month_name string,\n",
    "    enr_day integer,\n",
    "    enr_weekday integer,\n",
    "    enr_weekday_name string,\n",
    "    start_at timestamp,\n",
    "\tstop_at timestamp\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY(enr_year,enr_quarter,enr_month,bucket(10,enr_day))\n",
    ";\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE warehouse.gold.dim_rideable(\n",
    "    rideable_type_id integer,\n",
    "\trideable_type string\n",
    ")\n",
    "USING iceberg\n",
    ";\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fd06fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"SHOW tables in bronze.DW_ny_bike;\").show()\n",
    "# spark.sql(\"ALTER TABLE bronze.DW_ny_bike.trip_data_nybike ALTER COLUMN start_at TYPE string;\")\n",
    "# spark.sql(\"DROP TABLE bronze.DW_ny_bike.trip_data_nybike;\")\n",
    "spark.sql(\"TRUNCATE TABLE bronze.SylverDw_nybike.trip_data_nybike;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5272eb08-b01e-473f-a52b-08c16a491c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+-----------------+-----------------+-------------+\n",
      "|dw_period_tag|ride_id|start_station_id|  start_station_name|start_station_latitude|start_station_longitude|end_station_id|    end_station_name|end_station_latitude|end_station_longitude| user_type|gender|customer_year_birth|bike_id|rideable_type|         start_at|          stop_at|trip_duration|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+-----------------+-----------------+-------------+\n",
      "|         2016|   NULL|             312|Allen St & Stanto...|             40.722055|             -73.989111|           313|Washington Ave & ...|         40.69610226|         -73.96751037|Subscriber|     1|             1985.0|  22609|         NULL|9/1/2016 00:00:02|9/1/2016 00:16:18|          975|\n",
      "|         2016|   NULL|             316|Fulton St & Willi...|           40.70955958|           -74.00653609|           239|Willoughby St & F...|         40.69196566|          -73.9813018|Subscriber|     2|             1977.0|  16966|         NULL|9/1/2016 00:00:04|9/1/2016 00:20:25|         1220|\n",
      "|         2016|   NULL|             479|     9 Ave & W 45 St|           40.76019252|            -73.9912551|           448|    W 37 St & 10 Ave|         40.75660359|          -73.9979009|Subscriber|     1|             1983.0|  25601|         NULL|9/1/2016 00:00:19|9/1/2016 00:05:27|          308|\n",
      "|         2016|   NULL|             297|     E 15 St & 3 Ave|             40.734232|             -73.986923|           487| E 20 St & FDR Drive|         40.73314259|         -73.97573881|Subscriber|     1|             1953.0|  22094|         NULL|9/1/2016 00:00:25|9/1/2016 00:04:35|          250|\n",
      "|         2016|   NULL|             494|     W 26 St & 8 Ave|           40.74734825|           -73.99723551|           533|  Broadway & W 39 St|         40.75299641|         -73.98721619|Subscriber|     1|             1985.0|  16319|         NULL|9/1/2016 00:00:25|9/1/2016 00:07:45|          439|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+-----------------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"select * from warehouse.bronze.trip_data_nybike  limit 19;\").show()\n",
    "\n",
    "# spark.sql(\"select * from warehouse.sylver.trip_data_nybike where dw_period_tag='2016' limit 19;\").show(5)\n",
    "spark.sql(\"select * from warehouse.bronze.trip_data_nybike where dw_period_tag='2016' and month(start_at)=2 is null limit 19;\").show(5)\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# select \n",
    "#     *\n",
    "#     --count(*) \n",
    "#     --distinct(enr_user_type)\n",
    "# from warehouse.bronze.trip_data_nybike \n",
    "# # where \n",
    "# #     dw_period_tag = '2015'\n",
    "#     limit 10\n",
    "#     ;\"\"\"\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c383c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:=========================>                                (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"Drop table warehouse.sylver.trip_data_nybike ;\").show()\n",
    "# spark.sql(\"select distinct(dw_period_tag) from warehouse.sylver.trip_data_nybike  limit 19;\").show()\n",
    "# spark.sql(\"\"\"\n",
    "# select *\n",
    "# from warehouse.sylver.trip_data_nybike  \n",
    "# where month(start_at)=1\n",
    "# and year(start_at)=2015\n",
    "# limit 19;\n",
    "# \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select count(trip_uuid) as number\n",
    "from warehouse.sylver.trip_data_nybike  \n",
    "group by trip_uuid\n",
    "having count(trip_uuid) > 1\n",
    "limit 19;\n",
    "\"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# select \n",
    "# --count(*)\n",
    "# *\n",
    "# from  bronze.DW_ny_bike.trip_data_nybike \n",
    "# where dw_period_tag like '200408%' \n",
    "# limit 19;\n",
    "# \"\"\").show()\n",
    "# spark.sql(\"DELETE FROM bronze.DW_ny_bike.trip_data_nybike WHERE dw_period_tag='2022';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432fb18-2df1-4b13-baf5-bcb35de54e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b1961-c06f-4aa5-805a-f7b8dc8a6717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf20397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             name|\n",
      "+-----------------+\n",
      "|      Alex Merced|\n",
      "|Dipankar Mazumdar|\n",
      "|     Jason Hughes|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Insert Some Data\n",
    "spark.sql(\"INSERT INTO nessie.names VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2308fd5-e767-42aa-8b69-5649b40153d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "|dw_period_tag|ride_id|start_station_id|  start_station_name|start_station_latitude|start_station_longitude|end_station_id|    end_station_name|end_station_latitude|end_station_longitude| user_type|gender|customer_year_birth|bike_id|rideable_type|start_at|stop_at|trip_duration|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "|         2014|   NULL|             479|     9 Ave & W 45 St|           40.76019252|            -73.9912551|           540|Lexington Ave & E...|         40.74147286|         -73.98320928|Subscriber|     1|             1977.0|  21376|         NULL|    NULL|   NULL|       1027.0|\n",
      "|         2014|   NULL|             417|Barclay St & Chur...|           40.71291224|           -74.01020234|           417|Barclay St & Chur...|         40.71291224|         -74.01020234|Subscriber|     2|             1974.0|  16086|         NULL|    NULL|   NULL|        534.0|\n",
      "|         2014|   NULL|             327|Vesey Pl & River ...|            40.7153379|           -74.01658354|           415|Pearl St & Hanove...|          40.7047177|         -74.00926027|Subscriber|     1|             1990.0|  16073|         NULL|    NULL|   NULL|        416.0|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Query the Data\n",
    "spark.sql(\"SELECT * FROM nessie.dw_nybike.trip_data_nybike limit 3;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e4f1d9-d582-4668-b8d2-7988cce18c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame \n",
    "from typing import Optional\n",
    "from pyspark.sql.functions import lit,concat,to_timestamp,unix_timestamp , from_unixtime ,try_to_timestamp,col,month , udf,isnull ,when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType , TimestampType , DoubleType , FloatType, DateType,NullType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "bronze_schema_ny_bike = StructType([\n",
    "    StructField(\"dw_period_tag\", StringType(), nullable=True),\n",
    "    StructField(\"ride_id\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_id\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_name\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_latitude\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_longitude\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_id\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_name\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_latitude\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_longitude\", StringType(), nullable=True),\n",
    "    StructField(\"bike_id\", StringType(), nullable=True),\n",
    "    StructField(\"user_type\", StringType(), nullable=True),\n",
    "    StructField(\"gender\", StringType(), nullable=True),\n",
    "    StructField(\"customer_year_birth\", StringType(), nullable=True),\n",
    "    StructField(\"rideable_type\", StringType(), nullable=True),\n",
    "    StructField(\"start_at\", StringType(), nullable=True),\n",
    "    StructField(\"stop_at\", StringType(), nullable=True),\n",
    "    StructField(\"trip_duration\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "sylver_schema_ny_bike = StructType([\n",
    "    StructField(\"trip_uuid\", StringType(), nullable=True),\n",
    "    StructField(\"dw_period_tag\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_id\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_name\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_latitude\", StringType(), nullable=True),\n",
    "    StructField(\"start_station_longitude\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_id\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_name\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_latitude\", StringType(), nullable=True),\n",
    "    StructField(\"end_station_longitude\", StringType(), nullable=True),\n",
    "    StructField(\"bike_id\", StringType(), nullable=True),\n",
    "    StructField(\"enr_gender\", StringType(), nullable=True),\n",
    "    StructField(\"customer_year_birth\", StringType(), nullable=True),\n",
    "    StructField(\"rideable_type\", StringType(), nullable=True),\n",
    "    StructField(\"start_at\", TimestampType(), nullable=True),\n",
    "    StructField(\"stop_at\", TimestampType(), nullable=True),\n",
    "    StructField(\"trip_duration\", DoubleType(), nullable=True),\n",
    "    StructField(\"user_type\", StringType(), nullable=True),\n",
    "    StructField(\"enr_user_type\", StringType(), nullable=True),\n",
    "    StructField(\"year\", IntegerType(), nullable=True),\n",
    "    StructField(\"quarter\", IntegerType(), nullable=True),\n",
    "    StructField(\"quarter_name\", StringType(), nullable=True),\n",
    "    StructField(\"month\", IntegerType(), nullable=True),\n",
    "    StructField(\"month_name\", StringType(), nullable=True),\n",
    "    StructField(\"day\", IntegerType(), nullable=True),\n",
    "    StructField(\"weekday\", IntegerType(), nullable=True),\n",
    "    StructField(\"weekday_name\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c393fc1e-ec73-4a7d-a16b-9233a353dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def config_reader(path:str):\n",
    "    # Load YAML config\n",
    "    with open(path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "    \n",
    "def run(df,config):\n",
    "    print(\"Cast To Datamodel initiated\")\n",
    "    data_model_from_df = list(dict(df.dtypes).keys())\n",
    "\n",
    "    filtered_column = list(filter( lambda x: x not in data_model_from_df ,config['schema'].names ))\n",
    "    if len(filtered_column) > 0 :\n",
    "        for column in filtered_column:\n",
    "            df =  df.withColumn(column, lit(None))\n",
    "\n",
    "    df = df.select([\n",
    "        col(field.name).cast(field.dataType).alias(field.name) \n",
    "        for field in config['schema'].fields\n",
    "    ])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7502a4b-52fe-4de0-b731-cde010db868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bike_2015='/opt/airflow/data/2015-citibike-tripdata/1_January/*.csv'\n",
    "# file_bike_2017='/opt/airflow/data/2017-citibike-tripdata/*/*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80d02b77-63ff-4114-934c-fa63abb2a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_October   12_December  2_February  4_April  6_June  8_August\n",
      "11_November  1_January\t  3_March     5_May    7_July  9_September\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/airflow/data/2015-citibike-tripdata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a14fff5-9f09-41d1-b99f-cd58661e34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /opt/airflow/data/2015-citibike-tripdata/10_October/201510-citibike-tripdata_1.csv <==\n",
      "tripduration,starttime,stoptime,start station id,start station name,start station latitude,start station longitude,end station id,end station name,end station latitude,end station longitude,bikeid,usertype,birth year,gender\n",
      "171,10/1/2015 00:00:02,10/1/2015 00:02:54,388,W 26 St & 10 Ave,40.749717753,-74.002950346,494,W 26 St & 8 Ave,40.74734825,-73.99723551,24302,Subscriber,1973.0,1\n",
      "593,10/1/2015 00:00:02,10/1/2015 00:09:55,518,E 39 St & 2 Ave,40.74780373,-73.9734419,438,St Marks Pl & 1 Ave,40.72779126,-73.98564945,19904,Subscriber,1990.0,1\n",
      "233,10/1/2015 00:00:11,10/1/2015 00:04:05,447,8 Ave & W 52 St,40.76370739,-73.9851615,447,8 Ave & W 52 St,40.76370739,-73.9851615,17797,Subscriber,1984.0,1\n",
      "250,10/1/2015 00:00:15,10/1/2015 00:04:25,336,Sullivan St & Washington Sq,40.73047747,-73.99906065,223,W 13 St & 7 Ave,40.73781509,-73.99994661,23966,Subscriber,1984.0,1\n",
      "528,10/1/2015 00:00:17,10/1/2015 00:09:05,3107,Bedford Ave & Nassau Ave,40.72311651,-73.95212324,539,Metropolitan Ave & Bedford Ave,40.71534825,-73.96024116,16246,Customer,,0\n",
      "440,10/1/2015 00:00:17,10/1/2015 00:07:37,3107,Bedford Ave & Nassau Ave,40.72311651,-73.95212324,539,Metropolitan Ave & Bedford Ave,40.71534825,-73.96024116,23698,Customer,,0\n",
      "1185,10/1/2015 00:00:22,10/1/2015 00:20:07,531,Forsyth St & Broome St,40.71893904,-73.99266288,3064,Myrtle Ave & Lewis Ave,40.69681963,-73.93756926,17110,Subscriber,1987.0,1\n",
      "618,10/1/2015 00:00:25,10/1/2015 00:10:44,3002,South End Ave & Liberty St,40.711512,-74.015756,2004,6 Ave & Broome St,40.724399,-74.004704,16344,Subscriber,1989.0,1\n",
      "865,10/1/2015 00:00:31,10/1/2015 00:14:57,438,St Marks Pl & 1 Ave,40.72779126,-73.98564945,486,Broadway & W 29 St,40.7462009,-73.98855723,23822,Subscriber,1991.0,2\n",
      "\n",
      "==> /opt/airflow/data/2015-citibike-tripdata/10_October/201510-citibike-tripdata_2.csv <==\n",
      "tripduration,starttime,stoptime,start station id,start station name,start station latitude,start station longitude,end station id,end station name,end station latitude,end station longitude,bikeid,usertype,birth year,gender\n",
      "555,10/26/2015 13:10:10,10/26/2015 13:19:25,480,W 53 St & 10 Ave,40.76669671,-73.99061728,524,W 43 St & 6 Ave,40.75527307,-73.98316936,17359,Subscriber,1988.0,1\n",
      "818,10/26/2015 13:10:19,10/26/2015 13:23:58,281,Grand Army Plaza & Central Park S,40.7643971,-73.97371465,3148,E 84 St & 1 Ave,40.77565541,-73.95068615,19726,Subscriber,1993.0,1\n",
      "400,10/26/2015 13:10:18,10/26/2015 13:16:58,293,Lafayette St & E 8 St,40.73028666,-73.9907647,303,Mercer St & Spring St,40.72362738,-73.99949601,23696,Subscriber,1984.0,1\n",
      "513,10/26/2015 13:10:20,10/26/2015 13:18:53,362,Broadway & W 37 St,40.75172632,-73.98753523,435,W 21 St & 6 Ave,40.74173969,-73.99415556,23812,Subscriber,1979.0,1\n",
      "233,10/26/2015 13:10:21,10/26/2015 13:14:14,3158,W 63 St & Broadway,40.77163851,-73.98261428,468,Broadway & W 55 St,40.7652654,-73.98192338,24045,Subscriber,1987.0,1\n",
      "1081,10/26/2015 13:10:22,10/26/2015 13:28:23,519,Pershing Square North,40.751873,-73.977706,3144,E 81 St & Park Ave,40.77677702,-73.9590097,15974,Subscriber,1984.0,2\n",
      "728,10/26/2015 13:10:26,10/26/2015 13:22:35,447,8 Ave & W 52 St,40.76370739,-73.9851615,3159,W 67 St & Broadway,40.77492513,-73.98266566,19366,Subscriber,1989.0,1\n",
      "277,10/26/2015 13:10:29,10/26/2015 13:15:07,2008,Little West St & 1 Pl,40.70569254,-74.01677685,534,Water - Whitehall Plaza,40.70255065,-74.0127234,20641,Subscriber,1956.0,2\n",
      "268,10/26/2015 13:10:35,10/26/2015 13:15:04,448,W 37 St & 10 Ave,40.75660359,-73.9979009,458,11 Ave & W 27 St,40.751396,-74.005226,23379,Subscriber,1971.0,1\n"
     ]
    }
   ],
   "source": [
    "!head /opt/airflow/data/2015-citibike-tripdata/10_October/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40446034-e5c4-41a9-b3e0-44a9d2a3dc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5439a662-bed2-4c5c-b4d3-1ef5851d63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfinit =spark.read.format('csv').options(inferSchema='True', header='True',delimiter=',').load(file_bike_2015)\n",
    "# df2017 =spark.read.format('csv').options(header='True').load(file_bike_2017)\n",
    "dfinit= spark.sql(\"\"\"SELECT * FROM bronze.DW_ny_bike.trip_data_nybike WHERE dw_period_tag='2015'; \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f915952a-3cad-4143-8d21-f6c0925e30bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9937969"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfinit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbef5c56-bc6d-40e9-9e62-73dd05d999bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinit_2= spark.sql(\"\"\"SELECT * FROM bronze.DW_ny_bike.trip_data_nybike WHERE dw_period_tag='2015'; \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4004092-6428-45ae-b119-521348bd43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM bronze.DW_ny_bike.trip_data_nybike WHERE dw_period_tag='2015'; \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b9381ef-9b4d-4c42-b4eb-9d9ef9aba355",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=config_reader('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c9aad1-77f0-4e05-8643-1efdd9699bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+------------------+----------------------+-----------------------+--------------+----------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------------+--------------+-------------+\n",
      "|dw_period_tag|ride_id|start_station_id|start_station_name|start_station_latitude|start_station_longitude|end_station_id|end_station_name|end_station_latitude|end_station_longitude| user_type|gender|customer_year_birth|bike_id|rideable_type|      start_at|       stop_at|trip_duration|\n",
      "+-------------+-------+----------------+------------------+----------------------+-----------------------+--------------+----------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------------+--------------+-------------+\n",
      "|         2015|   NULL|             499|Broadway & W 60 St|           40.76915505|           -73.98191841|           423| W 54 St & 9 Ave|         40.76584941|         -73.98690506|Subscriber|     1|             1984.0|  15338|         NULL|1/18/2015 0:00|1/18/2015 0:05|          269|\n",
      "|         2015|   NULL|             164|   E 47 St & 2 Ave|           40.75323098|           -73.97032517|          2017| E 43 St & 2 Ave|         40.75022392|         -73.97121414|Subscriber|     1|             1954.0|  19103|         NULL|1/18/2015 0:01|1/18/2015 0:39|         2275|\n",
      "|         2015|   NULL|             529|   W 42 St & 8 Ave|            40.7575699|           -73.99098507|           478|11 Ave & W 41 St|         40.76030096|         -73.99884222|Subscriber|     1|             1981.0|  15653|         NULL|1/18/2015 0:01|1/18/2015 0:05|          223|\n",
      "+-------------+-------+----------------+------------------+----------------------+-----------------------+--------------+----------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+--------------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfinit_2.filter(dfinit_2.start_at.like('1/18/2015%')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c29c3bfd-0f63-4652-b8b2-5fadd8c9392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+-------------------+-------------------+-------------+\n",
      "|dw_period_tag|ride_id|start_station_id|  start_station_name|start_station_latitude|start_station_longitude|end_station_id|    end_station_name|end_station_latitude|end_station_longitude| user_type|gender|customer_year_birth|bike_id|rideable_type|           start_at|            stop_at|trip_duration|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+-------------------+-------------------+-------------+\n",
      "|         2015|   NULL|             497|  E 17 St & Broadway|           40.73704984|           -73.99009296|           518|     E 39 St & 2 Ave|         40.74780373|          -73.9734419|Subscriber|     2|             1979.0|  20184|         NULL|10/16/2015 00:00:06|10/16/2015 00:18:04|         1077|\n",
      "|         2015|   NULL|            3176|W 64 St & West En...|           40.77452835|           -73.98753759|          3175|W 70 St & Amsterd...|         40.77748046|         -73.98288594|Subscriber|     1|             1977.0|  24188|         NULL|10/16/2015 00:00:13|10/16/2015 00:07:21|          427|\n",
      "|         2015|   NULL|             512|     W 29 St & 9 Ave|            40.7500727|           -73.99839279|           536|     1 Ave & E 30 St|         40.74144387|         -73.97536082|Subscriber|     1|             1979.0|  15005|         NULL|10/16/2015 00:00:17|10/16/2015 00:10:52|          634|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+-------------------+-------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfinit_2.filter(dfinit_2.start_at.like('10/16/2015%')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "14628d30-8c25-4cda-b234-15b1a0da7eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cast To Datamodel initiated\n"
     ]
    }
   ],
   "source": [
    "config={'schema':sylver_schema_ny_bike}\n",
    "df_casted = run(dfinit,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "66377814-979a-48f0-93dc-eb209c60c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_uuid: string (nullable = true)\n",
      " |-- dw_period_tag: string (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: string (nullable = true)\n",
      " |-- start_station_longitude: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: string (nullable = true)\n",
      " |-- end_station_longitude: string (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- enr_gender: string (nullable = true)\n",
      " |-- customer_year_birth: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- start_at: timestamp (nullable = true)\n",
      " |-- stop_at: timestamp (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      " |-- customer_type: string (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- quarter_name: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- weekday_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "729fbeaf-9e5b-4ec7-8ecc-7adc1f1b6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_casted.select('*').when(isnull('start_at')).show(10)\n",
    "# df_casted.filter(month(col('start_at')) ==  ).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5522375-0652-4559-b55c-781e780bff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- start_at: string (nullable = true)\n",
      " |-- stop_at: string (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: double (nullable = true)\n",
      " |-- start_station_longitude: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: double (nullable = true)\n",
      " |-- end_station_longitude: double (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- customer_year_birth: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fabd9d38-e317-41fa-89ce-a960461408d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfconverted = df.to(bronze_schema_ny_bike)\n",
    "# df.select(df.start_at.cast(\"TIMESTAMP\").alias('start_at')).show()\n",
    "# df.select(to_timestamp(df.start_at, 'MM/dd/yyyy HH:mm:ss')).show(5)\n",
    "# df.select(from_unixtime(unix_timestamp('start_at', 'MM/dd/yyyy HH:mm:ss')).cast(TimestampType()).alias(\"timestamp\")).show(5)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime,to_timestamp,udf\n",
    "\n",
    "# df_renamed_v2 = df_renamed.select(from_unixtime(unix_timestamp('start_at', 'MM/dd/yyyy hh:mm:ss ')).cast(TimestampType()).alias(\"timestamp\"))\n",
    "# change timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2327764-65ab-43c3-bb52-6c66c3efac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed_v2 = df_renamed.withColumn(\"timestamp\", to_timestamp(\"start_at\", \"d/M/yyyy HH:mm:ss\"))\n",
    "# df = df.withColumn(\"timestamp\", to_timestamp(df[\"date_string\"], \"M/d/yyyy HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aedc22bb-1459-41f4-9141-9cd9827ca3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_uuid: string (nullable = true)\n",
      " |-- dw_period_tag: string (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: string (nullable = true)\n",
      " |-- start_station_longitude: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: string (nullable = true)\n",
      " |-- end_station_longitude: string (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- enr_gender: string (nullable = true)\n",
      " |-- customer_year_birth: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- start_at: timestamp (nullable = true)\n",
      " |-- stop_at: timestamp (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      " |-- customer_type: string (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- quarter_name: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- weekday_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4827c74a-c2b9-4780-afde-07f8dce7d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2014, 10, 1, 0, 0, 27)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "inDate = \"10/1/2014 00:00:27\"\n",
    "d = datetime.strptime(inDate, \"%m/%d/%Y %H:%M:%S\")\n",
    "d\n",
    "# strdate = d.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b8e2c462-a7fe-4b65-81d5-4e59d9a377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_guis=  d.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2e9f4863-6a9e-4e67-a5d6-7c809d22e59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str_guis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21586738-a80b-47fc-a1c6-a47bf6b2365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime(2013, 4, 29, 15, 59, 2)\n",
    "d.strftime(\"YYYYMMDD HH:mm:ss (%Y%m%d %H:%M:%S)\")\n",
    "'YYYYMMDD HH:mm:ss (20130429 15:59:02)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3d4ed6c7-3eaf-4587-94de-944a0602723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_udf = udf( lambda date: to_timestamp (datetime.strptime(date,'%m/%d/%Y %H:%M:%S'),TimestampType()))\n",
    "convert_udf = udf( lambda date: to_timestamp (col((datetime.strptime(date,'%m/%d/%Y %H:%M:%S').strftime(\"%Y-%m-%d %H:%M:%S\"),TimestampType()))))\n",
    "\n",
    "@udf\n",
    "def convert_to_timestam(date_in):\n",
    "    str_date = datetime.strptime(date_in,'%m/%d/%Y %H:%M:%S').strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # col_date = col(str_date)\n",
    "    # return to_timestamp(col_date)\n",
    "    return str_date\n",
    "\n",
    "@udf\n",
    "def convert_v2(date_in):\n",
    "    date_datetime_format = datetime.strptime(date_in,'%m/%d/%Y %H:%M:%S') \n",
    "    str_date = date_datetime_format.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return str_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a0136afa-3d31-41f5-bd20-470b3fa639ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+\n",
      "|tripduration|         starttime|          stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|\n",
      "+------------+------------------+------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+\n",
      "|        1027|10/1/2014 00:00:27|10/1/2014 00:17:34|             479|     9 Ave & W 45 St|           40.76019252|            -73.9912551|           540|Lexington Ave & E...|         40.74147286|         -73.98320928| 21376|Subscriber|    1977.0|     1|\n",
      "|         534|10/1/2014 00:00:36|10/1/2014 00:09:30|             417|Barclay St & Chur...|           40.71291224|           -74.01020234|           417|Barclay St & Chur...|         40.71291224|         -74.01020234| 16086|Subscriber|    1974.0|     2|\n",
      "+------------+------------------+------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dfinit.printSchema()\n",
    "dfinit.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e01bd45-c68a-44c7-ab91-8a5fc6064c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce,lit,concat,to_timestamp,unix_timestamp , from_unixtime ,try_to_timestamp,col,month , udf,isnull ,when,cast,to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType , TimestampType , DoubleType , FloatType, DateType,NullType\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fea40570-664d-4832-9542-e5dda81bd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1=dfinit.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7965f7c1-d262-459f-a0ad-005c9e52269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_start = row_1['starttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "66b0cd87-3975-4b45-b092-bca6e19afc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_startime = convert_v2(row_1['starttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549f188b-f144-4e7c-8d46-6b6da8bea83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+----------------+-------------+-------------+\n",
      "|dw_period_tag|ride_id|start_station_id|  start_station_name|start_station_latitude|start_station_longitude|end_station_id|    end_station_name|end_station_latitude|end_station_longitude| user_type|gender|customer_year_birth|bike_id|rideable_type|        start_at|      stop_at|trip_duration|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+----------------+-------------+-------------+\n",
      "|         2015|   NULL|             455|     1 Ave & E 44 St|           40.75001986|           -73.96905301|           265|Stanton St & Chry...|         40.72229346|         -73.99147535|Subscriber|     2|             1960.0|  18660|         NULL|1/1/2015 0:01:00|1/1/2015 0:24|         1346|\n",
      "|         2015|   NULL|             434|     9 Ave & W 18 St|           40.74317449|           -74.00366443|           482|     W 15 St & 7 Ave|         40.73935542|         -73.99931783|Subscriber|     1|             1963.0|  16085|         NULL|1/1/2015 0:02:00|1/1/2015 0:08|          363|\n",
      "|         2015|   NULL|             491|E 24 St & Park Ave S|           40.74096374|           -73.98602213|           505|     6 Ave & W 33 St|         40.74901271|         -73.98848395|Subscriber|     1|             1974.0|  20845|         NULL|1/1/2015 0:04:00|1/1/2015 0:10|          346|\n",
      "|         2015|   NULL|             384|Fulton St & Waver...|           40.68317813|            -73.9659641|           399|Lafayette Ave & S...|         40.68851534|          -73.9647628|Subscriber|     1|             1969.0|  19610|         NULL|1/1/2015 0:04:00|1/1/2015 0:07|          182|\n",
      "+-------------+-------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+----------+------+-------------------+-------+-------------+----------------+-------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfinit_2.filter(dfinit_2.start_at.like('1/1/2015%')).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae83f283-c8a5-4eab-9b4b-1f3df8cea840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfinit.select(convert_to_timestam(\"starttime\").alias(\"start_time\").cast(TimestampType())).printSchema()\n",
    "# dfinit.withColumn('starttime',convert_to_timestam(\"starttime\").cast(TimestampType())).printSchema()\n",
    "# dfinit.withColumn('start_at',when(to_timestamp(dfinit.start_at).isNotNull(),to_timestamp(dfinit.start_at)).when(convert_v2(col(\"start_at\")).isNotNull(),convert_v2(col(\"start_at\")))).show(2)\n",
    "# dfinit.withColumn('starttime',convert_v2('starttime')).withColumn('starttime',to_timestamp('starttime','yyyy-MM-dd HH:mm:ss')).printSchema()\n",
    "\n",
    "# dfinit.withColumn('starttime',when(to_timestamp(dfinit.starttime).isNotNull(),to_timestamp(dfinit.starttime)).when(convert_v2(col(\"starttime\")).isNotNull(),convert_v2(col(\"starttime\")))).withColumn('starttime',to_timestamp('starttime')).show(1)\n",
    "\n",
    "\n",
    "# dfinit.withColumn('starttime',to_timestamp(col('starttime'),'MM/d/yyyy HH:mm:ss')).filter(dfinit.starttime,isNull()).collect().show(1)\n",
    "# dfinit.withColumn('start_at',df['']('starttime')).show(1)\n",
    "\n",
    "# regex_pattern = r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{2}:\\d{2}:\\d{2}$\"\n",
    "# regex_pattern = r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{2}:\\d{2}$\"\n",
    "\n",
    "regex_pattern = r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}$\"\n",
    "column='start_at'\n",
    "\n",
    "dfinit_2=dfinit\\\n",
    ".filter(dfinit.start_at.like('1/%/2015%'))\\\n",
    ".withColumn(column,when(col(column).rlike(regex_pattern),concat(col(column),lit(':00'))).otherwise(col(column))) \\\n",
    "\n",
    "\n",
    "# .withColumn(\n",
    "#     \"enr_starttime\",\n",
    "#     coalesce(\n",
    "#         # to_timestamp(col(column), 'M/d/yyyy HH:mm:ss')\n",
    "#         to_timestamp(col(column), 'MM/d/yyyy H:mm:ss'),\n",
    "#         to_timestamp(col(column), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "#         to_timestamp(col(column))\n",
    "#     )\n",
    "# ).show(1)\n",
    "\n",
    "# dfinit.filter(dfinit.start_at.like('1/1/2015%')).withColumn(\"enr_starttime\",when(col('start_at').rlike(regex_pattern),concat(col('start_at'),lit(':00'))).otherwise(col('start_at'))).show(3)\n",
    "\n",
    "# dfinit.withColumn(\n",
    "#     \"enr_starttime\",\n",
    "#     when( col('starttime').rlike(regex_pattern),to_timestamp(col('starttime'))\n",
    "#         col('starttime').rlike(regex_pattern)\n",
    "# ).show(3)\n",
    "\n",
    "# dfinit.withColumn('starttime',to_timestamp(col('starttime'),'MM/d/yyyy HH:mm:ss')).filter(dfinit.starttime.isNull()).collect()\n",
    "# col(\"date_string\").rlike(regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3976814-5710-4bff-93cc-3871d81ead72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------------+------------------+----------------------+-----------------------+--------------+----------------+--------------------+---------------------+---------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "|dw_period_tag|ride_id|start_station_id|start_station_name|start_station_latitude|start_station_longitude|end_station_id|end_station_name|end_station_latitude|end_station_longitude|user_type|gender|customer_year_birth|bike_id|rideable_type|start_at|stop_at|trip_duration|\n",
      "+-------------+-------+----------------+------------------+----------------------+-----------------------+--------------+----------------+--------------------+---------------------+---------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "+-------------+-------+----------------+------------------+----------------------+-----------------------+--------------+----------------+--------------------+---------------------+---------+------+-------------------+-------+-------------+--------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfinit.filter(col('start_at').like('1/1/2015 __:__:__%')).show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "269c5657-3a62-4b36-97ee-ddd249435be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regex_pattern = r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}$\"\n",
    "# .filter(dfinit.start_at.rlike('1/%/2015% __:__:__'))\\\n",
    "\n",
    "@udf\n",
    "def safe_to_timestamp(column, fmt):\n",
    "    columnv2 = None\n",
    "    try:\n",
    "        columnv2 = when(to_timestamp(col(column), fmt).isNotNull(), to_timestamp(col(column), fmt)).otherwise(None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "    return columnv2\n",
    "\n",
    "d='file_parquet' \n",
    "# .filter(col('start_at').like('1/%/2015 _:__:__'))\\\n",
    "\n",
    "\n",
    "dfinit\\\n",
    ".withColumn(column,when(col(column).rlike(regex_pattern),concat(col(column),lit(':00'))).otherwise(col(column))) \\\n",
    ".withColumn(\n",
    "    \"enr_starttime\",\n",
    "    coalesce(\n",
    "        # safe_to_timestamp(col('start_at'), 'M/d/yyyy HH:mm:ss'),\n",
    "        # safe_to_timestamp(col('start_at'),'M/d/yyyy H:mm:ss')\n",
    "        to_timestamp(col(column), 'M/d/yyyy H:mm:ss'),\n",
    "        # to_timestamp(col(column), 'MM/dd/yyyy H:mm:ss')\n",
    "        # to_timestamp(col(column), 'MM/dd/yyyy H:mm:ss')\n",
    "        # to_timestamp(col(column), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        to_timestamp(col(column))\n",
    "    )\n",
    ").write.parquet(d, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da5f37-6f11-4b92-b280-53ec7206c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "    # Convert string dates to actual DateType\n",
    "    df = df.withColumn(\"startDate\", to_date(col(\"startDate\"), \"yyyy-MM-dd\")) \\\n",
    "           .withColumn(\"endDate\", to_date(col(\"endDate\"), \"yyyy-MM-dd\"))\n",
    "    # Show the DataFrame\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
